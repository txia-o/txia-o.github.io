<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on 水里面的博客</title><link>/tags/ai/</link><description>Recent content in AI on 水里面的博客</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sat, 30 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>【AI笔记】一、本地聊天大模型支持搜索引擎</title><link>/p/ai-note-1/</link><pubDate>Tue, 26 Mar 2024 00:00:00 +0000</pubDate><guid>/p/ai-note-1/</guid><description>&lt;p>最新闲来无事，用streamlit手撸了一个本地的聊天大模型，支持网页搜索。&lt;/p>
&lt;p>先放一张效果图：&lt;/p>
&lt;p>&lt;img src="/p/ai-note-1/demo.png"
width="1271"
height="1312"
srcset="/p/ai-note-1/demo_hu489fae0b987479a03d147f6e40c9a073_597071_480x0_resize_box_3.png 480w, /p/ai-note-1/demo_hu489fae0b987479a03d147f6e40c9a073_597071_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="聊天机器人对话截图"
class="gallery-image"
data-flex-grow="96"
data-flex-basis="232px"
>&lt;/p>
&lt;p>从日志可以看到是有先搜索再请求大模型：&lt;/p>
&lt;p>&lt;img src="/p/ai-note-1/backlog.png"
width="2707"
height="697"
srcset="/p/ai-note-1/backlog_huc3f1281fb959517b5dee0c086bf233c0_378842_480x0_resize_box_3.png 480w, /p/ai-note-1/backlog_huc3f1281fb959517b5dee0c086bf233c0_378842_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="后台日志"
class="gallery-image"
data-flex-grow="388"
data-flex-basis="932px"
>&lt;/p>
&lt;p>感觉还挺有意思，后续会持续优化一下，不知道能不能做到perplexity的水平。&lt;/p>
&lt;p>目前主要在考虑的优化方向：&lt;/p>
&lt;ul>
&lt;li>联系上下文的prompts&lt;/li>
&lt;li>模型调整：目前是千文的 14b-q8，考虑到上下文的处理，可能会使输入token变得非常大&lt;/li>
&lt;li>模型自动分析搜索结果及优化搜索query（有点难）&lt;/li>
&lt;/ul></description></item><item><title>【AI笔记】二、关于AI大模型量化的一些研究和总结</title><link>/p/ai-note-2/</link><pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate><guid>/p/ai-note-2/</guid><description>&lt;p>wip&lt;/p>
&lt;h2 id="引">
&lt;a href="#%e5%bc%95">#&lt;/a>
引
&lt;/h2>&lt;p>近日在网上冲浪时刷到了&lt;a class="link" href="https://mobiusml.github.io/1bit_blog/" target="_blank" rel="noopener"
>一篇文章&lt;/a>。这是一个关于大模型量化的研究团队发表的，这个团队提出他们使用了一种叫做hqq的量化技术，可以产生非常低精度的模型，并且实际推理效果超过一些较小的全精度模型。&lt;/p>
&lt;p>这让我对模型的量化稍微产生了一点兴趣。在我之前的探索中，我始终从huggingface上找到现成的模型，而没有尝试过自己量化。主要是觉得没有必要浪费算力和学习成本。不过现在我对此产生了一些兴趣，于是做了一些研究，总结如下：&lt;/p>
&lt;h2 id="1-传统的量化方法具体是如何实现的gptqggufawq">
&lt;a href="#1-%e4%bc%a0%e7%bb%9f%e7%9a%84%e9%87%8f%e5%8c%96%e6%96%b9%e6%b3%95%e5%85%b7%e4%bd%93%e6%98%af%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0%e7%9a%84gptqggufawq">#&lt;/a>
1. 传统的量化方法具体是如何实现的？GPTQ、GGUF、AWQ
&lt;/h2>&lt;h2 id="2-hqq和其他的方式之间的差异是什么他们的优势是什么">
&lt;a href="#2-hqq%e5%92%8c%e5%85%b6%e4%bb%96%e7%9a%84%e6%96%b9%e5%bc%8f%e4%b9%8b%e9%97%b4%e7%9a%84%e5%b7%ae%e5%bc%82%e6%98%af%e4%bb%80%e4%b9%88%e4%bb%96%e4%bb%ac%e7%9a%84%e4%bc%98%e5%8a%bf%e6%98%af%e4%bb%80%e4%b9%88">#&lt;/a>
2. HQQ和其他的方式之间的差异是什么？他们的优势是什么？
&lt;/h2>&lt;h2 id="参考">
&lt;a href="#%e5%8f%82%e8%80%83">#&lt;/a>
参考
&lt;/h2>&lt;p>&lt;a class="link" href="https://github.com/mobiusml/hqq" target="_blank" rel="noopener"
>HQQ - Github repo&lt;/a>
&lt;a class="link" href="https://zhuanlan.zhihu.com/p/644378038" target="_blank" rel="noopener"
>量化之后大模型的能力退化了多少 - 知乎&lt;/a>
&lt;a class="link" href="https://www.maartengrootendorst.com/blog/quantization/" target="_blank" rel="noopener"
>Which Quantization Method is Right for You? (GPTQ vs. GGUF vs. AWQ)&lt;/a>&lt;/p></description></item></channel></rss>